{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup embedding models and our vector database\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings()\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "openai_vector_store = FAISS.load_local(\"./data/nebraska-code\", openai_embeddings, \"openai\", allow_dangerous_deserialization=True)\n",
    "hf_vector_store = FAISS.load_local(\"./data/nebraska-code\", hf_embeddings, \"huggingface\", allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our LLMs\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "openai_llm = ChatOpenAI(  \n",
    "  model=\"gpt-3.5-turbo\",  \n",
    "  temperature=0.7\n",
    ")\n",
    "\n",
    "hf_llm = HuggingFaceEndpoint(\n",
    "  repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "  temperature=0.7,\n",
    "  huggingfacehub_api_token=os.getenv('HUGGINGFACE_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a local LLM\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "local_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt template\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful assistant helping attendees at Nebraska.Code(), a software development conference, make the most of their conference by answering questions about sessions and helping them find the best sessions to attend based on their interests. Your answers should be conversational - do not simply respond with the verbatim session data, but do make sure to include relevant information such as the room and times of the sessions. If there is more than one session that matches their query, feel free to reference them all. A bulleted list is acceptable in the output.\n",
    "Here are some sessions that appear relevant to the user's question. Use these as you see fit to help answer the user's question:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  template=template, \n",
    "  input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the chains\n",
    "\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "openai = (\n",
    "  {\"context\": openai_vector_store.as_retriever(search_kwargs={'k': 10}),  \"question\": RunnablePassthrough()} \n",
    "  | prompt \n",
    "  | openai_llm\n",
    "  | StrOutputParser() \n",
    ")\n",
    "\n",
    "hf = (\n",
    "  {\"context\": hf_vector_store.as_retriever(search_kwargs={'k': 10}),  \"question\": RunnablePassthrough()} \n",
    "  | prompt \n",
    "  | hf_llm\n",
    "  | StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(openai.invoke(\"It's time to start leaning AI. Which sessions should I attend?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hf.invoke(\"It's time to start leaning AI. Which sessions should I attend?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
